{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98eadea1",
   "metadata": {},
   "source": [
    "# Imports & Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3afe636",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard libraries\n",
    "import os\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.utils import make_grid\n",
    "\n",
    "# TensorBoard\n",
    "# from torch.utils.tensorboard import SummaryWriter\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "# Scikit-learn\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Kaggle dataset helper (if needed)\n",
    "import kagglehub  # Make sure this is actually used in your code\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Download latest version\n",
    "dataset_path = kagglehub.dataset_download(\"ebrahimelgazar/pixel-art\")\n",
    "\n",
    "print(\"Path to dataset files:\", dataset_path)\n",
    "csv_path = os.path.join(dataset_path, \"labels.csv\")\n",
    "sprites_path = os.path.join(dataset_path, \"sprites.npy\")\n",
    "sprite_label_path = os.path.join(dataset_path, \"sprites_labels.npy\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11479f73",
   "metadata": {},
   "source": [
    "# Generator Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "199881e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, latent_dim=100, num_classes=5, embed_dim=10):\n",
    "        super(Generator, self).__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        self.num_classes = num_classes\n",
    "        self.label_embedding = nn.Embedding(num_classes, embed_dim)\n",
    "        input_dim = latent_dim + embed_dim\n",
    "\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            # Dense -> 4x4x256\n",
    "            nn.Linear(input_dim, 4 * 4 * 256, bias=False),\n",
    "            nn.BatchNorm1d(4 * 4 * 256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "            # Reshape to (batch_size, 256, 4, 4)\n",
    "            View((-1, 256, 4, 4)),\n",
    "            \n",
    "            # Upsampling 1: 4x4 -> 8x8\n",
    "            nn.ConvTranspose2d(256, 128, kernel_size=5, stride=2, padding=2, output_padding=1, bias=False),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "            # Upsampling 2: 8x8 -> 16x16\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=5, stride=2, padding=2, output_padding=1, bias=False),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "            # Output layer: 16x16x3\n",
    "            nn.ConvTranspose2d(64, 3, kernel_size=5, stride=1, padding=2, bias=False),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        \n",
    "    def forward(self, noise, labels):\n",
    "        label_embed = self.label_embedding(labels)  # shape: (batch_size, embed_dim)\n",
    "        x = torch.cat([noise, label_embed], dim=1)  # concatenate along features\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "# Helper module to reshape tensors\n",
    "class View(nn.Module):\n",
    "    def __init__(self, shape):\n",
    "        super(View, self).__init__()\n",
    "        self.shape = shape  # shape: tuple (-1, C, H, W)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x.view(self.shape)\n",
    "\n",
    "\n",
    "# Example usage\n",
    "latent_dim = 100\n",
    "num_classes = 5\n",
    "batch_size = 8\n",
    "gen = Generator(latent_dim, num_classes)\n",
    "z = torch.randn(batch_size, latent_dim)\n",
    "labels = torch.randint(0, num_classes, (batch_size,))  # random class labels\n",
    "fake_imgs = gen(z, labels)\n",
    "print(fake_imgs.shape)  # should be [8, 3, 16, 16]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f50a544",
   "metadata": {},
   "source": [
    "# Discriminator Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2dfb036",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, num_classes=5, embed_dim=10):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        # Label embedding\n",
    "        self.label_embedding = nn.Embedding(num_classes, embed_dim)\n",
    "        \n",
    "        # Convolutional layers\n",
    "        self.model = nn.Sequential(\n",
    "            # Input: (3 + embed_dim) x 16 x 16 -> 64 x 8 x 8\n",
    "            nn.Conv2d(3 + embed_dim, 64, kernel_size=5, stride=2, padding=2),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Dropout(0.3),\n",
    "            \n",
    "            # 8x8x64 -> 4x4x128\n",
    "            nn.Conv2d(64, 128, kernel_size=5, stride=2, padding=2),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Dropout(0.3),\n",
    "            \n",
    "            # 4x4x128 -> 2x2x256\n",
    "            nn.Conv2d(128, 256, kernel_size=5, stride=2, padding=2),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Dropout(0.3),\n",
    "            \n",
    "            nn.Flatten(),\n",
    "            nn.Linear(256 * 2 * 2, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x, labels):\n",
    "        \"\"\"\n",
    "        x: images (batch_size, 3, 16, 16)\n",
    "        labels: class indices (batch_size,)\n",
    "        \"\"\"\n",
    "        batch_size = x.size(0)\n",
    "        \n",
    "        # Embed labels and expand to match spatial size\n",
    "        label_embed = self.label_embedding(labels)  # (batch_size, embed_dim)\n",
    "        label_embed = label_embed.unsqueeze(2).unsqueeze(3)  # (batch, embed_dim, 1, 1)\n",
    "        label_embed = label_embed.expand(batch_size, label_embed.size(1), 16, 16)\n",
    "        \n",
    "        # Concatenate image and label embedding\n",
    "        x = torch.cat([x, label_embed], dim=1)  # (batch, 3 + embed_dim, 16, 16)\n",
    "        \n",
    "        return self.model(x)\n",
    "\n",
    "# Example usage\n",
    "num_classes = 5\n",
    "batch_size = 8\n",
    "disc = Discriminator(num_classes)\n",
    "x = torch.randn(batch_size, 3, 16, 16)\n",
    "labels = torch.randint(0, num_classes, (batch_size,))\n",
    "output = disc(x, labels)\n",
    "print(output.shape)  # [8, 1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca24bf03",
   "metadata": {},
   "source": [
    "# Loss Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d347d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binary Cross Entropy Loss\n",
    "bce_loss = nn.BCELoss()\n",
    "\n",
    "def discriminator_loss(real_output, fake_output):\n",
    "    \"\"\"\n",
    "    Discriminator loss\n",
    "    real_output: discriminator predictions on real images\n",
    "    fake_output: discriminator predictions on fake images\n",
    "    \"\"\"\n",
    "    # Labels\n",
    "    real_labels = torch.ones_like(real_output)\n",
    "    fake_labels = torch.zeros_like(fake_output)\n",
    "    \n",
    "    real_loss = bce_loss(real_output, real_labels)\n",
    "    fake_loss = bce_loss(fake_output, fake_labels)\n",
    "    total_loss = real_loss + fake_loss\n",
    "    return total_loss\n",
    "\n",
    "def generator_loss(fake_output):\n",
    "    \"\"\"\n",
    "    Generator loss\n",
    "    fake_output: discriminator predictions on generated images\n",
    "    \"\"\"\n",
    "    labels = torch.ones_like(fake_output)  # Generator wants discriminator to predict 1\n",
    "    loss = bce_loss(fake_output, labels)\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ab90ca8",
   "metadata": {},
   "source": [
    "# Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aafcdf97",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PixelArtDataset(Dataset):\n",
    "    def __init__(self, images_path, labels_path=None):\n",
    "        # Load images\n",
    "        self.images = np.load(images_path)\n",
    "        labels = np.load(labels_path)\n",
    "        class_labels = np.argmax(labels, axis=1) \n",
    "        self.labels = class_labels \n",
    "        # Validate images\n",
    "        if self.images.shape[1:] != (16, 16, 3):\n",
    "            raise ValueError(f\"Images must be 16x16x3, but are {self.images.shape}\")\n",
    "        if self.images.dtype != np.uint8:\n",
    "            raise TypeError(f\"Images must be uint8, but are {self.images.dtype}\")\n",
    "\n",
    "        # Normalize images to [-1, 1] and convert to float32\n",
    "        self.images = (self.images.astype(np.float32) - 127.5) / 127.5\n",
    "\n",
    "        # Convert to PyTorch tensors and change shape to (C, H, W)\n",
    "        self.images = torch.from_numpy(self.images).permute(0, 3, 1, 2)\n",
    "        \n",
    "        print(f\"Dataset loaded: {len(self.images)} images\")\n",
    "        print(f\"Image shape: {self.images.shape}\")\n",
    "        print(f\"Image range: [{self.images.min():.3f}, {self.images.max():.3f}]\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.labels is not None:\n",
    "            return self.images[idx], self.labels[idx]\n",
    "        return self.images[idx], 0  # return dummy label for compatibility\n",
    "\n",
    "# Usage example\n",
    "batch_size = 32\n",
    "dataset = PixelArtDataset(sprites_path, sprite_label_path)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db393967",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PixelArtGAN:\n",
    "    def __init__(self, generator, discriminator, generator_optimizer, discriminator_optimizer, latent_dim=100, num_classes=5, writer=None):\n",
    "        self.generator = generator.to(DEVICE)\n",
    "        self.discriminator = discriminator.to(DEVICE)\n",
    "        self.latent_dim = latent_dim\n",
    "        self.num_classes = num_classes\n",
    "        self.discriminator_optimizer = discriminator_optimizer\n",
    "        self.generator_optimizer = generator_optimizer\n",
    "        self.writer = writer\n",
    "        \n",
    "        # Fixed noise for monitoring\n",
    "        self.seed = torch.randn(num_classes, latent_dim, device=DEVICE)\n",
    "\n",
    "        # Fixed labels: one-hot encoding for each class\n",
    "        self.seed_labels = torch.arange(num_classes, device=DEVICE)\n",
    "\n",
    "\n",
    "    def train_step(self, real_images, labels):\n",
    "        batch_size = real_images.size(0)\n",
    "        real_images = real_images.to(DEVICE)\n",
    "        labels = labels.to(DEVICE)\n",
    "        \n",
    "        \n",
    "        # Generate noise\n",
    "        noise = torch.randn(batch_size, self.latent_dim, device=DEVICE)\n",
    "        \n",
    "        # Train Generator\n",
    "        self.generator_optimizer.zero_grad()\n",
    "        generated_images = self.generator(noise, labels)\n",
    "        fake_output = self.discriminator(generated_images, labels)\n",
    "        g_loss = bce_loss(fake_output, torch.ones_like(fake_output))\n",
    "        g_loss.backward()\n",
    "        self.generator_optimizer.step()\n",
    "        \n",
    "        # Train Discriminator\n",
    "        self.discriminator_optimizer.zero_grad()\n",
    "        real_output = self.discriminator(real_images, labels)\n",
    "        fake_output_detached = self.discriminator(generated_images.detach(), labels)\n",
    "        real_loss = bce_loss(real_output, torch.ones_like(real_output))\n",
    "        fake_loss = bce_loss(fake_output_detached, torch.zeros_like(fake_output_detached))\n",
    "        d_loss = real_loss + fake_loss\n",
    "        d_loss.backward()\n",
    "        self.discriminator_optimizer.step()\n",
    "        \n",
    "        return g_loss.item(), d_loss.item()\n",
    "    \n",
    "    def generate_and_show_images(self, epoch):\n",
    "        \"\"\"Generate images and log them to TensorBoard (one per class)\"\"\"\n",
    "        self.generator.eval()\n",
    "        with torch.no_grad():\n",
    "            preds = self.generator(self.seed, self.seed_labels).cpu()  # generated images\n",
    "\n",
    "        # Rescale from [-1,1] -> [0,1]\n",
    "        preds = (preds + 1) / 2\n",
    "        preds = torch.clamp(preds, 0, 1)\n",
    "\n",
    "        if self.writer is not None:\n",
    "            # Make a grid of images\n",
    "            grid = torchvision.utils.make_grid(preds, nrow=self.num_classes)\n",
    "            self.writer.add_image(\"Generated Images\", grid, epoch)\n",
    "        \n",
    "        self.generator.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7fb77bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(generator, discriminator, g_optimizer, d_optimizer, epoch, path):\n",
    "    checkpoint = {\n",
    "        \"generator\": generator.state_dict(),\n",
    "        \"discriminator\": discriminator.state_dict(),\n",
    "        \"g_optimizer\": g_optimizer.state_dict(),\n",
    "        \"d_optimizer\": d_optimizer.state_dict(),\n",
    "        \"epoch\": epoch\n",
    "    }\n",
    "    torch.save(checkpoint, path)\n",
    "    print(f\"âœ… Checkpoint saved at {path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffb4fc9d",
   "metadata": {},
   "source": [
    "# Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f14a4f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_gan(\n",
    "    gan,\n",
    "    dataloader,\n",
    "    epochs,\n",
    "    save_interval=10,\n",
    "    checkpoint_dir=\"checkpoints\",\n",
    "    writer=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Train GAN model\n",
    "    \n",
    "    Args:\n",
    "        gan: PixelArtGAN instance\n",
    "        dataloader: torch DataLoader with (images, labels)\n",
    "        epochs: int, number of epochs\n",
    "        generator_optimizer: torch.optim optimizer for generator\n",
    "        discriminator_optimizer: torch.optim optimizer for discriminator\n",
    "        save_interval: int, save checkpoint every N epochs\n",
    "        checkpoint_dir: str, path to save checkpoints\n",
    "        writer: TensorBoard SummaryWriter (optional)\n",
    "    \"\"\"\n",
    "    \n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "    \n",
    "    gen_losses, disc_losses = [], []\n",
    "    print(\"ðŸš€ Starting training...\")\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        epoch_g_loss, epoch_d_loss = [], []\n",
    "\n",
    "        loop = tqdm(dataloader, desc=f\"Epoch {epoch+1}/{epochs}\")\n",
    "        for images, labels in loop:\n",
    "            g_loss, d_loss = gan.train_step(images, labels)\n",
    "            epoch_g_loss.append(g_loss)\n",
    "            epoch_d_loss.append(d_loss)\n",
    "            loop.set_postfix({'g_loss': g_loss, 'd_loss': d_loss})\n",
    "\n",
    "        avg_g_loss = sum(epoch_g_loss) / len(epoch_g_loss)\n",
    "        avg_d_loss = sum(epoch_d_loss) / len(epoch_d_loss)\n",
    "        gen_losses.append(avg_g_loss)\n",
    "        disc_losses.append(avg_d_loss)\n",
    "\n",
    "        # Log to TensorBoard\n",
    "        if writer is not None:\n",
    "            writer.add_scalar(\"Generator Loss\", avg_g_loss, epoch+1)\n",
    "            writer.add_scalar(\"Discriminator Loss\", avg_d_loss, epoch+1)\n",
    "            gan.generate_and_show_images(epoch+1)\n",
    "            \n",
    "\n",
    "        # Print and generate images\n",
    "        print(f\"Epoch {epoch+1}/{epochs} - Gen Loss: {avg_g_loss:.4f}, Disc Loss: {avg_d_loss:.4f}\")\n",
    "        \n",
    "\n",
    "        # Save checkpoints periodically\n",
    "        if (epoch + 1) % save_interval == 0:\n",
    "            save_path = os.path.join(checkpoint_dir, f\"epoch_{epoch+1}.pth\")\n",
    "            save_checkpoint(\n",
    "                gan.generator,\n",
    "                gan.discriminator,\n",
    "                gan.generator_optimizer,\n",
    "                gan.discriminator_optimizer,\n",
    "                epoch+1,\n",
    "                save_path\n",
    "            )\n",
    "\n",
    "    return gen_losses, disc_losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef813ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GAN hyperparameters\n",
    "LATENT_DIM = 100\n",
    "LEARNING_RATE = 0.0002\n",
    "BETA_1 = 0.5\n",
    "EPOCHS = 200\n",
    "BATCH_SIZE = 32\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62861d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TensorBoard writer\n",
    "current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "run_name = f\"pixel_art_gan_{current_time}\"\n",
    "\n",
    "# Set the writer with a custom event name\n",
    "writer = SummaryWriter(log_dir=f\"runs/{run_name}\")\n",
    "\n",
    "# Optimizers\n",
    "g_optimizer = optim.Adam(gen.parameters(), lr=LEARNING_RATE, betas=(0.5, 0.999))\n",
    "d_optimizer = optim.Adam(disc.parameters(), lr=LEARNING_RATE, betas=(0.5, 0.999))\n",
    "\n",
    "# Initialize GAN wrapper\n",
    "gan = PixelArtGAN(\n",
    "    generator=gen,\n",
    "    discriminator=disc,\n",
    "    generator_optimizer=g_optimizer,\n",
    "    discriminator_optimizer=d_optimizer,\n",
    "    latent_dim=LATENT_DIM,\n",
    "    num_classes=5,\n",
    "    writer=writer,\n",
    ")\n",
    "\n",
    "# Train GAN\n",
    "gen_losses, disc_losses = train_gan(\n",
    "    gan=gan,\n",
    "    dataloader=dataloader,\n",
    "    epochs=EPOCHS,\n",
    "    save_interval=10,\n",
    "    checkpoint_dir=f\"checkpoints/GAN_{current_time}\",\n",
    "    writer=writer,\n",
    ")\n",
    "\n",
    "writer.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
