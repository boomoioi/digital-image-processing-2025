{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports & Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "1b11297f-ca63-44e7-9aae-f161bf299eec",
    "_uuid": "5705d11d-e757-4920-86ba-15d20d7665a4",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-02-14T18:12:16.442936Z",
     "iopub.status.busy": "2024-02-14T18:12:16.442236Z",
     "iopub.status.idle": "2024-02-14T18:12:23.237857Z",
     "shell.execute_reply": "2024-02-14T18:12:23.236846Z",
     "shell.execute_reply.started": "2024-02-14T18:12:16.442901Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import models, transforms\n",
    "from torchvision.utils import save_image, make_grid\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.animation import FuncAnimation, PillowWriter\n",
    "import numpy as np\n",
    "from IPython.display import HTML\n",
    "from torchvision.utils import save_image, make_grid\n",
    "import os\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1deb06c-68ad-4902-90d3-43555aebe13b",
    "_kg_hide-input": true,
    "_uuid": "d088289e-8f7c-4ff8-8b72-ec59ba99368b",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-02-14T18:12:23.242339Z",
     "iopub.status.busy": "2024-02-14T18:12:23.241921Z",
     "iopub.status.idle": "2024-02-14T18:12:23.290920Z",
     "shell.execute_reply": "2024-02-14T18:12:23.289888Z",
     "shell.execute_reply.started": "2024-02-14T18:12:23.242311Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class ResidualConvBlock(nn.Module):\n",
    "    def __init__(\n",
    "        self, in_channels: int, out_channels: int, is_res: bool = False\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        # Check if input and output channels are the same for the residual connection\n",
    "        self.same_channels = in_channels == out_channels\n",
    "\n",
    "        # Flag for whether or not to use residual connection\n",
    "        self.is_res = is_res\n",
    "\n",
    "        # First convolutional layer\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels, out_channels, 3, 1, 1\n",
    "            ),  # 3x3 kernel with stride 1 and padding 1\n",
    "            nn.BatchNorm2d(out_channels),  # Batch normalization\n",
    "            nn.GELU(),  # GELU activation function\n",
    "        )\n",
    "\n",
    "        # Second convolutional layer\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                out_channels, out_channels, 3, 1, 1\n",
    "            ),  # 3x3 kernel with stride 1 and padding 1\n",
    "            nn.BatchNorm2d(out_channels),  # Batch normalization\n",
    "            nn.GELU(),  # GELU activation function\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "\n",
    "        # If using residual connection\n",
    "        if self.is_res:\n",
    "            # Apply first convolutional layer\n",
    "            x1 = self.conv1(x)\n",
    "\n",
    "            # Apply second convolutional layer\n",
    "            x2 = self.conv2(x1)\n",
    "\n",
    "            # If input and output channels are the same, add residual connection directly\n",
    "            if self.same_channels:\n",
    "                out = x + x2\n",
    "            else:\n",
    "                # If not, apply a 1x1 convolutional layer to match dimensions before adding residual connection\n",
    "                shortcut = nn.Conv2d(\n",
    "                    x.shape[1], x2.shape[1], kernel_size=1, stride=1, padding=0\n",
    "                ).to(x.device)\n",
    "                out = shortcut(x) + x2\n",
    "            # print(f\"resconv forward: x {x.shape}, x1 {x1.shape}, x2 {x2.shape}, out {out.shape}\")\n",
    "\n",
    "            # Normalize output tensor\n",
    "            return out / 1.414\n",
    "\n",
    "        # If not using residual connection, return output of second convolutional layer\n",
    "        else:\n",
    "            x1 = self.conv1(x)\n",
    "            x2 = self.conv2(x1)\n",
    "            return x2\n",
    "\n",
    "    # Method to get the number of output channels for this block\n",
    "    def get_out_channels(self):\n",
    "        return self.conv2[0].out_channels\n",
    "\n",
    "    # Method to set the number of output channels for this block\n",
    "    def set_out_channels(self, out_channels):\n",
    "        self.conv1[0].out_channels = out_channels\n",
    "        self.conv2[0].in_channels = out_channels\n",
    "        self.conv2[0].out_channels = out_channels\n",
    "\n",
    "\n",
    "class UnetUp(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(UnetUp, self).__init__()\n",
    "\n",
    "        # Create a list of layers for the upsampling block\n",
    "        # The block consists of a ConvTranspose2d layer for upsampling, followed by two ResidualConvBlock layers\n",
    "        layers = [\n",
    "            nn.ConvTranspose2d(in_channels, out_channels, 2, 2),\n",
    "            ResidualConvBlock(out_channels, out_channels),\n",
    "            ResidualConvBlock(out_channels, out_channels),\n",
    "        ]\n",
    "\n",
    "        # Use the layers to create a sequential model\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x, skip):\n",
    "        # Concatenate the input tensor x with the skip connection tensor along the channel dimension\n",
    "        x = torch.cat((x, skip), 1)\n",
    "\n",
    "        # Pass the concatenated tensor through the sequential model and return the output\n",
    "        x = self.model(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class UnetDown(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(UnetDown, self).__init__()\n",
    "\n",
    "        # Create a list of layers for the downsampling block\n",
    "        # Each block consists of two ResidualConvBlock layers, followed by a MaxPool2d layer for downsampling\n",
    "        layers = [\n",
    "            ResidualConvBlock(in_channels, out_channels),\n",
    "            ResidualConvBlock(out_channels, out_channels),\n",
    "            nn.MaxPool2d(2),\n",
    "        ]\n",
    "\n",
    "        # Use the layers to create a sequential model\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Pass the input through the sequential model and return the output\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "class EmbedFC(nn.Module):\n",
    "    def __init__(self, input_dim, emb_dim):\n",
    "        super(EmbedFC, self).__init__()\n",
    "        \"\"\"\n",
    "        This class defines a generic one layer feed-forward neural network for embedding input data of\n",
    "        dimensionality input_dim to an embedding space of dimensionality emb_dim.\n",
    "        \"\"\"\n",
    "        self.input_dim = input_dim\n",
    "\n",
    "        # define the layers for the network\n",
    "        layers = [\n",
    "            nn.Linear(input_dim, emb_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(emb_dim, emb_dim),\n",
    "        ]\n",
    "\n",
    "        # create a PyTorch sequential model consisting of the defined layers\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # flatten the input tensor\n",
    "        x = x.view(-1, self.input_dim)\n",
    "        # apply the model layers to the flattened tensor\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "def unorm(x):\n",
    "    # unity norm. results in range of [0,1]\n",
    "    # assume x (h,w,3)\n",
    "    xmax = x.max((0, 1))\n",
    "    xmin = x.min((0, 1))\n",
    "    return (x - xmin) / (xmax - xmin)\n",
    "\n",
    "\n",
    "def norm_all(store, n_t, n_s):\n",
    "    # runs unity norm on all timesteps of all samples\n",
    "    nstore = np.zeros_like(store)\n",
    "    for t in range(n_t):\n",
    "        for s in range(n_s):\n",
    "            nstore[t, s] = unorm(store[t, s])\n",
    "    return nstore\n",
    "\n",
    "\n",
    "def norm_torch(x_all):\n",
    "    # runs unity norm on all timesteps of all samples\n",
    "    # input is (n_samples, 3,h,w), the torch image format\n",
    "    x = x_all.cpu().numpy()\n",
    "    xmax = x.max((2, 3))\n",
    "    xmin = x.min((2, 3))\n",
    "    xmax = np.expand_dims(xmax, (2, 3))\n",
    "    xmin = np.expand_dims(xmin, (2, 3))\n",
    "    nstore = (x - xmin) / (xmax - xmin)\n",
    "    return torch.from_numpy(nstore)\n",
    "\n",
    "\n",
    "def gen_tst_context(n_cfeat):\n",
    "    \"\"\"\n",
    "    Generate test context vectors\n",
    "    \"\"\"\n",
    "    vec = torch.tensor(\n",
    "        [\n",
    "            [1, 0, 0, 0, 0],\n",
    "            [0, 1, 0, 0, 0],\n",
    "            [0, 0, 1, 0, 0],\n",
    "            [0, 0, 0, 1, 0],\n",
    "            [0, 0, 0, 0, 1],\n",
    "            [0, 0, 0, 0, 0],  # human, non-human, food, spell, side-facing\n",
    "            [1, 0, 0, 0, 0],\n",
    "            [0, 1, 0, 0, 0],\n",
    "            [0, 0, 1, 0, 0],\n",
    "            [0, 0, 0, 1, 0],\n",
    "            [0, 0, 0, 0, 1],\n",
    "            [0, 0, 0, 0, 0],  # human, non-human, food, spell, side-facing\n",
    "            [1, 0, 0, 0, 0],\n",
    "            [0, 1, 0, 0, 0],\n",
    "            [0, 0, 1, 0, 0],\n",
    "            [0, 0, 0, 1, 0],\n",
    "            [0, 0, 0, 0, 1],\n",
    "            [0, 0, 0, 0, 0],  # human, non-human, food, spell, side-facing\n",
    "            [1, 0, 0, 0, 0],\n",
    "            [0, 1, 0, 0, 0],\n",
    "            [0, 0, 1, 0, 0],\n",
    "            [0, 0, 0, 1, 0],\n",
    "            [0, 0, 0, 0, 1],\n",
    "            [0, 0, 0, 0, 0],  # human, non-human, food, spell, side-facing\n",
    "            [1, 0, 0, 0, 0],\n",
    "            [0, 1, 0, 0, 0],\n",
    "            [0, 0, 1, 0, 0],\n",
    "            [0, 0, 0, 1, 0],\n",
    "            [0, 0, 0, 0, 1],\n",
    "            [0, 0, 0, 0, 0],  # human, non-human, food, spell, side-facing\n",
    "            [1, 0, 0, 0, 0],\n",
    "            [0, 1, 0, 0, 0],\n",
    "            [0, 0, 1, 0, 0],\n",
    "            [0, 0, 0, 1, 0],\n",
    "            [0, 0, 0, 0, 1],\n",
    "            [0, 0, 0, 0, 0],\n",
    "        ]  # human, non-human, food, spell, side-facing\n",
    "    )\n",
    "    return len(vec), vec\n",
    "\n",
    "\n",
    "def plot_grid(x, n_sample, n_rows, save_dir, w):\n",
    "    # x:(n_sample, 3, h, w)\n",
    "    ncols = n_sample // n_rows\n",
    "    grid = make_grid(\n",
    "        norm_torch(x), nrow=ncols\n",
    "    )  # curiously, nrow is number of columns.. or number of items in the row.\n",
    "    save_image(grid, save_dir + f\"run_image_w{w}.png\")\n",
    "    print(\"saved image at \" + save_dir + f\"run_image_w{w}.png\")\n",
    "    return grid\n",
    "\n",
    "\n",
    "def plot_sample(x_gen_store, n_sample, nrows, save_dir, fn, w, save=False):\n",
    "    ncols = n_sample // nrows\n",
    "    sx_gen_store = np.moveaxis(\n",
    "        x_gen_store, 2, 4\n",
    "    )  # change to Numpy image format (h,w,channels) vs (channels,h,w)\n",
    "    nsx_gen_store = norm_all(\n",
    "        sx_gen_store, sx_gen_store.shape[0], n_sample\n",
    "    )  # unity norm to put in range [0,1] for np.imshow\n",
    "\n",
    "    # create gif of images evolving over time, based on x_gen_store\n",
    "    fig, axs = plt.subplots(\n",
    "        nrows=nrows, ncols=ncols, sharex=True, sharey=True, figsize=(ncols, nrows)\n",
    "    )\n",
    "\n",
    "    def animate_diff(i, store):\n",
    "        print(f\"gif animating frame {i} of {store.shape[0]}\", end=\"\\r\")\n",
    "        plots = []\n",
    "        for row in range(nrows):\n",
    "            for col in range(ncols):\n",
    "                axs[row, col].clear()\n",
    "                axs[row, col].set_xticks([])\n",
    "                axs[row, col].set_yticks([])\n",
    "                plots.append(axs[row, col].imshow(store[i, (row * ncols) + col]))\n",
    "        return plots\n",
    "\n",
    "    ani = FuncAnimation(\n",
    "        fig,\n",
    "        animate_diff,\n",
    "        fargs=[nsx_gen_store],\n",
    "        interval=200,\n",
    "        blit=False,\n",
    "        repeat=True,\n",
    "        frames=nsx_gen_store.shape[0],\n",
    "    )\n",
    "    plt.close()\n",
    "    if save:\n",
    "        ani.save(save_dir + f\"{fn}_w{w}.gif\", dpi=100, writer=PillowWriter(fps=5))\n",
    "        print(\"saved gif at \" + save_dir + f\"{fn}_w{w}.gif\")\n",
    "    return ani\n",
    "\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, sfilename, lfilename, transform, null_context=False):\n",
    "        self.sprites = np.load(sfilename)\n",
    "        labels = np.load(lfilename)\n",
    "        class_labels = np.argmax(labels, axis=1)\n",
    "        self.slabels = class_labels\n",
    "        print(f\"sprite shape: {self.sprites.shape}\")\n",
    "        print(f\"labels shape: {self.slabels.shape}\")\n",
    "        self.transform = transform\n",
    "        self.null_context = null_context\n",
    "        self.sprites_shape = self.sprites.shape\n",
    "        self.slabel_shape = self.slabels.shape\n",
    "\n",
    "    # Return the number of images in the dataset\n",
    "    def __len__(self):\n",
    "        return len(self.sprites)\n",
    "\n",
    "    # Get the image and label at a given index\n",
    "    def __getitem__(self, idx):\n",
    "        # Return the image and label as a tuple\n",
    "        if self.transform:\n",
    "            image = self.transform(self.sprites[idx])\n",
    "            if self.null_context:\n",
    "                label = torch.tensor(0).to(torch.int64)\n",
    "            else:\n",
    "                label = torch.tensor(self.slabels[idx]).to(torch.int64)\n",
    "        return (image, label)\n",
    "\n",
    "    def getshapes(self):\n",
    "        # return shapes of data and labels\n",
    "        return self.sprites_shape, self.slabel_shape\n",
    "\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToTensor(),  # from [0,255] to range [0.0,1.0]\n",
    "        transforms.Normalize((0.5,), (0.5,)),  # range [-1,1]\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "0c795920-1597-42f4-ab62-354711621c89",
    "_uuid": "ca01534e-af7e-42ef-aaae-65859959a1fc",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-02-14T18:12:23.293079Z",
     "iopub.status.busy": "2024-02-14T18:12:23.292656Z",
     "iopub.status.idle": "2024-02-14T18:12:23.310743Z",
     "shell.execute_reply": "2024-02-14T18:12:23.309784Z",
     "shell.execute_reply.started": "2024-02-14T18:12:23.293044Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class ContextUnet(nn.Module):\n",
    "    def __init__(\n",
    "        self, in_channels, n_feat=256, n_cfeat=10, height=28\n",
    "    ):  # cfeat - context features\n",
    "        super(ContextUnet, self).__init__()\n",
    "\n",
    "        # number of input channels, number of intermediate feature maps and number of classes\n",
    "        self.in_channels = in_channels\n",
    "        self.n_feat = n_feat\n",
    "        self.n_cfeat = n_cfeat\n",
    "        self.h = height  # assume h == w. must be divisible by 4, so 28,24,20,16...\n",
    "\n",
    "        # Initialize the initial convolutional layer\n",
    "        self.init_conv = ResidualConvBlock(in_channels, n_feat, is_res=True)\n",
    "\n",
    "        # Initialize the down-sampling path of the U-Net with two levels\n",
    "        self.down1 = UnetDown(n_feat, n_feat)  # down1 #[10, 256, 8, 8]\n",
    "        self.down2 = UnetDown(n_feat, 2 * n_feat)  # down2 #[10, 512, 4,  4]\n",
    "\n",
    "        # original: self.to_vec = nn.Sequential(nn.AvgPool2d(7), nn.GELU())\n",
    "        self.to_vec = nn.Sequential(nn.AvgPool2d((4)), nn.GELU())\n",
    "\n",
    "        # Embed the timestep and context labels with a one-layer fully connected neural network\n",
    "        self.timeembed1 = EmbedFC(1, 2 * n_feat)\n",
    "        self.timeembed2 = EmbedFC(1, 1 * n_feat)\n",
    "        self.contextembed1 = EmbedFC(n_cfeat, 2 * n_feat)\n",
    "        self.contextembed2 = EmbedFC(n_cfeat, 1 * n_feat)\n",
    "\n",
    "        # Initialize the up-sampling path of the U-Net with three levels\n",
    "        self.up0 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(\n",
    "                2 * n_feat, 2 * n_feat, self.h // 4, self.h // 4\n",
    "            ),  # up-sample\n",
    "            nn.GroupNorm(8, 2 * n_feat),  # normalize\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.up1 = UnetUp(4 * n_feat, n_feat)\n",
    "        self.up2 = UnetUp(2 * n_feat, n_feat)\n",
    "\n",
    "        # Initialize the final convolutional layers to map to the same number of channels as the input image\n",
    "        self.out = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                2 * n_feat, n_feat, 3, 1, 1\n",
    "            ),  # reduce number of feature maps   #in_channels, out_channels, kernel_size, stride=1, padding=0\n",
    "            nn.GroupNorm(8, n_feat),  # normalize\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(\n",
    "                n_feat, self.in_channels, 3, 1, 1\n",
    "            ),  # map to same number of channels as input\n",
    "        )\n",
    "\n",
    "    def forward(self, x, t, c=None):\n",
    "        \"\"\"\n",
    "        x : (batch, n_feat, h, w) : input image\n",
    "        t : (batch, n_cfeat)      : time step\n",
    "        c : (batch, n_classes)    : context label\n",
    "        \"\"\"\n",
    "        # x is the input image, c is the context label, t is the timestep, context_mask says which samples to block the context on\n",
    "\n",
    "        # pass the input image through the initial convolutional layer\n",
    "        x = self.init_conv(x)\n",
    "        # pass the result through the down-sampling path\n",
    "        down1 = self.down1(x)  # [10, 256, 8, 8]\n",
    "        down2 = self.down2(down1)  # [10, 256, 4, 4]\n",
    "\n",
    "        # convert the feature maps to a vector and apply an activation\n",
    "        hiddenvec = self.to_vec(down2)\n",
    "\n",
    "        # mask out context if context_mask == 1\n",
    "        if c is None:\n",
    "            c = torch.zeros(x.shape[0], self.n_cfeat).to(x)\n",
    "\n",
    "        # embed context and timestep\n",
    "        cemb1 = self.contextembed1(c).view(\n",
    "            -1, self.n_feat * 2, 1, 1\n",
    "        )  # (batch, 2*n_feat, 1,1)\n",
    "        temb1 = self.timeembed1(t).view(-1, self.n_feat * 2, 1, 1)\n",
    "        cemb2 = self.contextembed2(c).view(-1, self.n_feat, 1, 1)\n",
    "        temb2 = self.timeembed2(t).view(-1, self.n_feat, 1, 1)\n",
    "        # print(f\"uunet forward: cemb1 {cemb1.shape}. temb1 {temb1.shape}, cemb2 {cemb2.shape}. temb2 {temb2.shape}\")\n",
    "\n",
    "        up1 = self.up0(hiddenvec)\n",
    "        up2 = self.up1(cemb1 * up1 + temb1, down2)  # add and multiply embeddings\n",
    "        up3 = self.up2(cemb2 * up2 + temb2, down1)\n",
    "        out = self.out(torch.cat((up3, x), 1))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "9d3a032b-91cd-4569-8ce4-ba65ff187027",
    "_uuid": "5770d69c-b5c8-489c-a516-1ed4153d5a82",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-02-14T18:12:23.314543Z",
     "iopub.status.busy": "2024-02-14T18:12:23.313873Z",
     "iopub.status.idle": "2024-02-14T18:12:23.376310Z",
     "shell.execute_reply": "2024-02-14T18:12:23.374975Z",
     "shell.execute_reply.started": "2024-02-14T18:12:23.314512Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "\n",
    "# diffusion hyperparameters\n",
    "timesteps = 500\n",
    "beta1 = 1e-4\n",
    "beta2 = 0.02\n",
    "\n",
    "# network hyperparameters\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else torch.device(\"cpu\"))\n",
    "n_feat = 64  # 64 hidden dimension feature\n",
    "n_cfeat = 5  # context vector is of size 5\n",
    "height = 16  # 16x16 image\n",
    "save_dir = \"tmp/\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "# training hyperparameters\n",
    "batch_size = 100\n",
    "n_epoch = 100\n",
    "lrate = 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "25aa91c5-8582-4f80-a2e7-1a4057839eea",
    "_uuid": "18323c40-14ed-4ddb-9b0b-2487fcbe3f04",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-02-14T18:12:23.378144Z",
     "iopub.status.busy": "2024-02-14T18:12:23.377813Z",
     "iopub.status.idle": "2024-02-14T18:12:23.764664Z",
     "shell.execute_reply": "2024-02-14T18:12:23.763819Z",
     "shell.execute_reply.started": "2024-02-14T18:12:23.378115Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# construct DDPM noise schedule\n",
    "b_t = (beta2 - beta1) * torch.linspace(0, 1, timesteps + 1, device=device) + beta1\n",
    "a_t = 1 - b_t\n",
    "ab_t = torch.cumsum(a_t.log(), dim=0).exp()\n",
    "ab_t[0] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "1a1c3371-ab6c-41df-88dd-43ecf7ccd746",
    "_uuid": "9b9c1d57-e9fa-4000-b36a-0d5bc92318ec",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-02-14T18:12:23.766678Z",
     "iopub.status.busy": "2024-02-14T18:12:23.765880Z",
     "iopub.status.idle": "2024-02-14T18:12:23.823147Z",
     "shell.execute_reply": "2024-02-14T18:12:23.822243Z",
     "shell.execute_reply.started": "2024-02-14T18:12:23.766641Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "nn_model = ContextUnet(in_channels=3, n_feat=n_feat, n_cfeat=n_cfeat, height=height).to(\n",
    "    device\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kagglehub\n",
    "\n",
    "dataset_path = kagglehub.dataset_download(\"ebrahimelgazar/pixel-art\")\n",
    "\n",
    "print(\"Path to dataset files:\", dataset_path)\n",
    "csv_path = os.path.join(dataset_path, \"labels.csv\")\n",
    "sprites_path = os.path.join(dataset_path, \"sprites.npy\")\n",
    "sprite_label_path = os.path.join(dataset_path, \"sprites_labels.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "bf1d138f-7d0c-41af-9282-a81f919dd627",
    "_uuid": "7ec7bfd4-1b14-46e4-884b-8e329b47e732",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-02-14T18:12:23.824545Z",
     "iopub.status.busy": "2024-02-14T18:12:23.824259Z",
     "iopub.status.idle": "2024-02-14T18:12:24.323208Z",
     "shell.execute_reply": "2024-02-14T18:12:24.322297Z",
     "shell.execute_reply.started": "2024-02-14T18:12:23.824519Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "dataset = CustomDataset(sprites_path, sprite_label_path, transform, null_context=False)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "optim = torch.optim.Adam(nn_model.parameters(), lr=lrate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "c42e9bb5-0e01-4837-b850-41904ec87fc6",
    "_uuid": "b146ef9b-f82a-401f-856a-a696877e2de0",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-02-14T18:12:24.325298Z",
     "iopub.status.busy": "2024-02-14T18:12:24.324617Z",
     "iopub.status.idle": "2024-02-14T18:12:24.332988Z",
     "shell.execute_reply": "2024-02-14T18:12:24.331911Z",
     "shell.execute_reply.started": "2024-02-14T18:12:24.325256Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# helper function: perturbs an image to a specified noise level\n",
    "def perturb_input(x, t, noise):\n",
    "    return (\n",
    "        ab_t.sqrt()[t, None, None, None] * x + (1 - ab_t[t, None, None, None]) * noise\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function; removes the predicted noise (but adds some noise back in to avoid collapse)\n",
    "def denoise_add_noise(x, t, pred_noise, z=None):\n",
    "    if z is None:\n",
    "        z = torch.randn_like(x)\n",
    "    noise = b_t.sqrt()[t] * z\n",
    "    mean = (x - pred_noise * ((1 - a_t[t]) / (1 - ab_t[t]).sqrt())) / a_t[t].sqrt()\n",
    "    return mean + noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def sample_images_per_class(n_sample_per_class=5):\n",
    "    all_samples = []\n",
    "    for class_idx in range(n_cfeat):\n",
    "        samples = torch.randn(n_sample_per_class, 3, height, height, device=device)\n",
    "        c = torch.zeros(n_sample_per_class, n_cfeat, device=device)\n",
    "        c[:, class_idx] = 1.0\n",
    "\n",
    "        for i in range(timesteps, 0, -1):\n",
    "            t_embed = torch.full((n_sample_per_class, 1), i / timesteps, device=device)\n",
    "            z = torch.randn_like(samples) if i > 1 else 0\n",
    "            eps = nn_model(samples, t_embed, c=c)\n",
    "            samples = denoise_add_noise(samples, i, eps, z)\n",
    "\n",
    "        all_samples.append(samples.cpu())\n",
    "    # shape: [n_classes, n_samples, C, H, W]\n",
    "    return torch.cat(all_samples, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "860027cd-e748-4a0a-b3f3-2bc68860418a",
    "_kg_hide-output": true,
    "_uuid": "bb1e120b-b3d6-4975-bdbc-15a94dc6ee2f",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-02-14T18:12:24.334453Z",
     "iopub.status.busy": "2024-02-14T18:12:24.334182Z",
     "iopub.status.idle": "2024-02-14T18:35:07.285008Z",
     "shell.execute_reply": "2024-02-14T18:35:07.283803Z",
     "shell.execute_reply.started": "2024-02-14T18:12:24.334429Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "# from torch.utils.tensorboard import SummaryWriter\n",
    "from tensorboardX import SummaryWriter\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "def train_diffusion_model(\n",
    "    nn_model,\n",
    "    dataloader,\n",
    "    optim,\n",
    "    device,\n",
    "    timesteps,\n",
    "    n_cfeat,\n",
    "    n_epoch,\n",
    "    lrate,\n",
    "    save_dir,\n",
    "    writer=None,\n",
    "):\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    global_step = 0\n",
    "\n",
    "    for ep in range(n_epoch):\n",
    "        nn_model.train()\n",
    "        # linearly decay learning rate\n",
    "        lr = lrate * (1 - ep / n_epoch)\n",
    "        optim.param_groups[0][\"lr\"] = lr\n",
    "\n",
    "        epoch_loss = 0\n",
    "        pbar = tqdm(dataloader, desc=f\"Epoch {ep+1}/{n_epoch}\", mininterval=2)\n",
    "\n",
    "        for x, label in pbar:\n",
    "            optim.zero_grad()\n",
    "            x = x.to(device)\n",
    "\n",
    "            # perturb data\n",
    "            noise = torch.randn_like(x)\n",
    "            t = torch.randint(1, timesteps + 1, (x.shape[0],), device=device)\n",
    "            x_pert = perturb_input(x, t, noise)\n",
    "\n",
    "            y_onehot = F.one_hot(label, num_classes=n_cfeat).float().to(device)\n",
    "            pred_noise = nn_model(x_pert, t / timesteps, c=y_onehot)\n",
    "            \n",
    "            loss = F.mse_loss(pred_noise, noise)\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "\n",
    "            batch_loss = loss.item()\n",
    "            epoch_loss += batch_loss\n",
    "            pbar.set_postfix(loss=batch_loss, lr=lr)\n",
    "\n",
    "            # log batch loss\n",
    "            if writer:\n",
    "                writer.add_scalar(\"Loss/batch\", batch_loss, global_step)\n",
    "                writer.add_scalar(\"LR\", lr, global_step)\n",
    "            global_step += 1\n",
    "\n",
    "        # log epoch loss\n",
    "        avg_epoch_loss = epoch_loss / len(dataloader)\n",
    "\n",
    "        print(f\"Epoch {ep+1}/{n_epoch}, Avg Loss: {avg_epoch_loss:.6f}\")\n",
    "\n",
    "        # save model periodically\n",
    "        if (ep + 1) % 10 == 0 or (ep + 1) == n_epoch:\n",
    "            save_path = os.path.join(save_dir, f\"epoch_{ep+1}.pth\")\n",
    "            torch.save(nn_model.state_dict(), save_path)\n",
    "            print(f\"Saved model at {save_path}\")\n",
    "\n",
    "        nn_model.eval()\n",
    "        if writer:\n",
    "            samples_to_log = sample_images_per_class(\n",
    "                n_sample_per_class=1\n",
    "            )  # 5 per class\n",
    "            writer.add_scalar(\"Loss/epoch\", avg_epoch_loss, ep)\n",
    "            grid = make_grid(samples_to_log, nrow=5, normalize=True, scale_each=True)\n",
    "            writer.add_image(\"Generated Images\", grid, ep)\n",
    "\n",
    "    print(\"Training complete!\")\n",
    "\n",
    "\n",
    "writer = SummaryWriter(log_dir=\"runs\")\n",
    "train_diffusion_model(\n",
    "    nn_model,\n",
    "    dataloader,\n",
    "    optim,\n",
    "    device,\n",
    "    timesteps,\n",
    "    n_cfeat,\n",
    "    n_epoch,\n",
    "    lrate,\n",
    "    save_dir,\n",
    "    writer=writer,\n",
    ")\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "aaf0ba9f-962f-4e69-b55c-99d771c2eba0",
    "_uuid": "660b4592-ab0c-40db-8144-b0fca7455042",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-02-14T18:35:07.298191Z",
     "iopub.status.busy": "2024-02-14T18:35:07.297817Z",
     "iopub.status.idle": "2024-02-14T18:35:07.308524Z",
     "shell.execute_reply": "2024-02-14T18:35:07.307467Z",
     "shell.execute_reply.started": "2024-02-14T18:35:07.298161Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# sample using standard algorithm\n",
    "@torch.no_grad()\n",
    "def sample_ddpm(n_sample, class_idx=None, save_rate=20):\n",
    "    \"\"\"\n",
    "    n_sample : number of images to generate\n",
    "    class_idx : integer specifying class to condition on (0..n_cfeat-1), None for unconditional\n",
    "    \"\"\"\n",
    "    samples = torch.randn(n_sample, 3, height, height).to(device)\n",
    "\n",
    "    # create class vector if conditioning\n",
    "    if class_idx is not None:\n",
    "        c = torch.zeros(n_sample, n_cfeat, device=device)\n",
    "        c[:, class_idx] = 1.0\n",
    "    else:\n",
    "        c = None\n",
    "\n",
    "    intermediate = []\n",
    "\n",
    "    for i in range(timesteps, 0, -1):\n",
    "        print(f\"sampling timestep {i:3d}\", end=\"\\r\")\n",
    "        t_embed = torch.full((n_sample, 1), i / timesteps, device=device)\n",
    "        z = torch.randn_like(samples) if i > 1 else 0\n",
    "        eps = nn_model(samples, t_embed, c=c)\n",
    "        samples = denoise_add_noise(samples, i, eps, z)\n",
    "        if i % save_rate == 0 or i == timesteps or i < 8:\n",
    "            intermediate.append(samples.detach().cpu().numpy())\n",
    "\n",
    "    intermediate = np.stack(intermediate)\n",
    "    return samples, intermediate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "71260547-fa36-443e-9fa5-fc447d7c8fea",
    "_uuid": "ad13d481-981b-4f3c-b983-64b5ed0cadd0",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-02-14T18:35:07.310524Z",
     "iopub.status.busy": "2024-02-14T18:35:07.309879Z",
     "iopub.status.idle": "2024-02-14T18:35:07.353755Z",
     "shell.execute_reply": "2024-02-14T18:35:07.352790Z",
     "shell.execute_reply.started": "2024-02-14T18:35:07.310450Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# load in model weights and set to eval mode\n",
    "nn_model.load_state_dict(torch.load(f\"{save_dir}/model.pth\", map_location=device))\n",
    "nn_model.eval()\n",
    "print(\"Loaded in Model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import HTML\n",
    "\n",
    "plt.clf()\n",
    "class_idx = 2  # choose the class you want to generate\n",
    "samples, intermediate_ddpm = sample_ddpm(32, class_idx=0)\n",
    "\n",
    "animation_ddpm = plot_sample(\n",
    "    intermediate_ddpm,\n",
    "    n_sample=32,\n",
    "    nrows=4,\n",
    "    save_dir=save_dir,\n",
    "    fn=f\"ani_run_class{class_idx}\",\n",
    "    w=None,\n",
    "    save=False,\n",
    ")\n",
    "\n",
    "HTML(animation_ddpm.to_jshtml())"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 4424547,
     "sourceId": 7600561,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30646,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
